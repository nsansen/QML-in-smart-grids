{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb251ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quantum\\anaconda3\\envs\\Quantum\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (28792, 10, 1) (28792,)\n",
      "Validation data shape: (5758, 10, 1) (5758,)\n",
      "Testing data shape: (3840, 10, 1) (3840,)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 10)                480       \n",
      "                                                                 \n",
      " keras_layer (KerasLayer)    (None, 10)                0 (unused)\n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 491\n",
      "Trainable params: 491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/12\n",
      "212/900 [======>.......................] - ETA: 12:46 - loss: 3.4754 - mse: 3.4754 - MAE: 1.8542 - MAPE: 56057924.0000 - accuracy: 0.0404"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import pandas as pd\n",
    "from pennylane import numpy as np\n",
    "import pennylane as qml\n",
    "import tensorflow as tf\n",
    "###################################################################################################\n",
    "###################################################################################################\n",
    "Epochs=12\n",
    "n_qubits = 10#Number of qubits should be the same as number of features, max number = 25\n",
    "blocks = 1 #Νumber of blocks (AngleEmbedding and StronglyEntanglingLayers is one block )\n",
    "layers = 4  #layers per block (multiple “layers” of StronglyEntanglingLayers per block )\n",
    "copy=\"1\" #Number to be set for title\n",
    "model_name = \"H3t_R\" + str(copy) +\"_E\" + str(Epochs) + \"Q\" + str(n_qubits) + \"B\" + str(blocks) + \"L\" + str(layers)\n",
    "batch_size = 32\n",
    "LR=0.0001 # Learning rate\n",
    "splitsequence = 10 #more than 10 i.e. 20\n",
    "patience= 100 #For early stopping\n",
    "###################################################################################################\n",
    "# Set the seed for weights initialization\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "###################################################################################################\n",
    "\n",
    "dev = qml.device(\"default.qubit.tf\", wires=n_qubits) #Run the model in classical CPU\n",
    "# dev = qml.device(\"qiskit.aer\", wires=n_qubits, shots=1000)\n",
    "\n",
    "# Set the split ratios\n",
    "train_ratio = 0.75  # 75% for training\n",
    "val_ratio = 0.15  # 15% for validation\n",
    "#Target\n",
    "ms = 'MS701'\n",
    "#%%\n",
    "df = pd.read_csv('data_quantum.csv').iloc[:,1:]\n",
    "#df.index=pd.to_datetime(df[\"Unnamed: 0\"], format = \"%Y-%m-%d %H:%M:%S\") \n",
    "d1 = df[ms]\n",
    "from lstm_data_preparation import lstm_data\n",
    "x0, y0 = lstm_data(d1,ms,splitsequence)\n",
    "x0 = x0.reshape(x0.shape[0],x0.shape[1],1)\n",
    "#%%\n",
    "\n",
    "# Calculate the lengths of each set\n",
    "total_length = len(x0)\n",
    "train_length = int(total_length * train_ratio)\n",
    "val_length = int(total_length * val_ratio)\n",
    "test_length = total_length - train_length - val_length\n",
    "# Split the data\n",
    "x_train = x0[:train_length]\n",
    "y_train = y0[:train_length]\n",
    "x_val = x0[train_length:train_length+val_length]\n",
    "y_val = y0[train_length:train_length+val_length]\n",
    "x_test = x0[train_length+val_length:]\n",
    "y_test = y0[train_length+val_length:]\n",
    "# Print the shapes of the datasets\n",
    "print(\"Training data shape:\", x_train.shape, y_train.shape)\n",
    "print(\"Validation data shape:\", x_val.shape, y_val.shape)\n",
    "print(\"Testing data shape:\", x_test.shape, y_test.shape)\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Preprocess the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# Scale the training dataset\n",
    "x_train_scaled = scaler.fit_transform(x_train.reshape(-1, 1)).reshape(x_train.shape)\n",
    "# Scale the validation dataset\n",
    "x_val_scaled = scaler.fit_transform(x_val.reshape(-1, 1)).reshape(x_val.shape)\n",
    "# Scale the test dataset\n",
    "x_test_scaled = scaler.fit_transform(x_test.reshape(-1, 1)).reshape(x_test.shape)\n",
    "y_train_scaled =  scaler.fit_transform(y_train.reshape(-1, 1)).reshape(y_train.shape)\n",
    "y_val_scaled =  scaler.fit_transform(y_val.reshape(-1, 1)).reshape(y_val.shape)\n",
    "y_test_scaled =  scaler.fit_transform(y_test.reshape(-1, 1)).reshape(y_test.shape)\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "\n",
    "# Define quantum node\n",
    "# Added below to ensure the QNode is JIT compiled\n",
    "@tf.function\n",
    "@qml.qnode(dev, interface=\"tf\", diff_method=\"backprop\")\n",
    "def qnode(inputs, weights):\n",
    "    for i in range(blocks):\n",
    "        qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))\n",
    "        qml.templates.StronglyEntanglingLayers(weights[i], wires=range(n_qubits)) #STRONGLY ENTANGLING LAYERS\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "weights_shape = (blocks, layers, n_qubits, 3) # Uncomment for Strongly entangling layers\n",
    "tf.keras.backend.set_floatx(\"float32\")\n",
    "weight_shapes = {\"weights\": weights_shape}\n",
    "\n",
    "####### Set seed for constant weights #######\n",
    "inputs = tf.constant(np.random.random((batch_size, n_qubits))) #uncomment for random weights\n",
    "#rng = tf.random.Generator.from_seed(seed)\n",
    "#random_numbers = rng.uniform(shape=(batch_size, n_qubits))\n",
    "#inputs = tf.constant(random_numbers)\n",
    "######\n",
    "\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from matplotlib import pyplot\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "#from keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import r2_score\n",
    "#%%\n",
    "import time\n",
    "\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n",
    "        \n",
    "time_callback = TimeHistory()\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=LR) \n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    tf.keras.backend.set_floatx(\"float32\")\n",
    "    weight_shapes = {\"weights\": weights_shape}\n",
    "    layer_1 = tf.keras.layers.LSTM(10,  activation='tanh', input_shape=(x0.shape[1],x0.shape[2]),dtype=tf.float32) ### input shape: timesteps, features\n",
    "    layer_2 = tf.keras.layers.Dense(32, activation='tanh',  dtype=tf.float32)\n",
    "    #layer_3 = tf.keras.layers.Dense(64, activation='linear', dtype=tf.float32)\n",
    "    #layer_4 = tf.keras.layers.Dense(8, activation='linear', dtype=tf.float32)\n",
    "    layer_P = tf.keras.layers.Dense(n_qubits, activation=\"tanh\", dtype=tf.float32)\n",
    "    qlayer =  qml.qnn.KerasLayer(qnode, weight_shapes, n_qubits, dtype=tf.float32) \n",
    "    #qlayer = tf.keras.layers.Dense(n_qubits, activation=\"tanh\", dtype=tf.float32)\n",
    "    layer_D = tf.keras.layers.Dense(1, activation=\"linear\")\n",
    "    model1 = tf.keras.models.Sequential([layer_1,qlayer,layer_D])\n",
    "    \n",
    "   # layer_4=keras.layers.TimeDistributed(Dense(4, activation='relu'))\n",
    "    \n",
    "    \n",
    "\n",
    "#Compile model           \n",
    "#model1.compile(loss='mse', optimizer=opt, metrics=['mean_absolute_error',\"MAPE\",\"MAE\"])\n",
    "model1.compile(loss='mse', optimizer=opt, metrics=['mse',\"MAE\",\"MAPE\",\"accuracy\"])\n",
    "                              \n",
    "# Define the EarlyStopping callback\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',  mode='min',patience=patience, verbose=1)\n",
    "# Define the Model checkpoint callback\n",
    "#cp2 = ModelCheckpoint(\"model1/\", monitor='val_loss',save_best_only=True)\n",
    "#Fit model\n",
    "print(model1.summary())\n",
    "history = model1.fit(x_train_scaled, y_train_scaled, epochs=Epochs, batch_size=batch_size\n",
    "                     ,validation_data=(x_val_scaled, y_val_scaled), callbacks=[time_callback,early_stopping],shuffle=False)\n",
    "#%%\n",
    "\n",
    "# Save the training history to a file\n",
    "#history_file = model_name + \"_history.txt\"\n",
    "#with open(history_file, 'w') as file:\n",
    "#    file.write(str(history.history))\n",
    "\n",
    "history_file = model_name + \"_Summary.txt\"\n",
    "with open(history_file, 'w') as file:\n",
    "    # Get the summary of the model architecture as a string\n",
    "    summary_string = []\n",
    "    model1.summary(print_fn=lambda x: summary_string.append(x))\n",
    "# Process the summary string into a pandas DataFrame\n",
    "    summary_data = [x.split() for x in summary_string[1:-1]]\n",
    "    summary_df = pd.DataFrame(summary_data, columns=['Layer', 'Output','Shape', 'Param',\"type\",\"#\"])\n",
    "    summary_df[\"blocks\"]=blocks\n",
    "    summary_df[\"layers\"]=layers\n",
    "    summary_df[\"LR\"]=LR\n",
    "    summary_df[\"batch_size\"]=batch_size\n",
    "    summary_df[\"Epochs\"]=Epochs\n",
    "    summary_df[\"N_qubits\"]=n_qubits\n",
    "    summary_df[\"train_ratio\"]=train_ratio\n",
    "    summary_df[\"val_ratio\"]=val_ratio\n",
    "    summary_df[\"seed\"]=seed\n",
    "    summary_df[\"splitsequence\"]=splitsequence\n",
    "    summary_df[\"patience\"]=patience\n",
    "    summary_df[\"MS\"]=ms\n",
    "    file.write(str(summary_df[['Output','Param','blocks','layers',\n",
    "                               'LR','batch_size','Epochs','N_qubits','train_ratio',\n",
    "                               'val_ratio']].dropna()))\n",
    "# make predictions Scaled and unscaled\n",
    "pred_scaled = model1.predict(x_test_scaled)\n",
    "pred_unscaled = scaler.inverse_transform(pred_scaled.reshape(-1, 1)).reshape(pred_scaled.shape)\n",
    "print(\"------------------\")\n",
    "print(\"Test MSE:\", mean_squared_error(y_test, pred_unscaled))\n",
    "print(\"Test RMSE:\", np.sqrt(mean_squared_error(y_test, pred_unscaled)))\n",
    "print(\"Test MAE:\", mean_absolute_error(y_test, pred_unscaled))\n",
    "print(\"Test MAPE:\", mean_absolute_percentage_error(y_test,pred_unscaled))\n",
    "print(\"R2 Score:\", r2_score(y_test, pred_unscaled, multioutput='variance_weighted'))\n",
    "print(\"------------------\")\n",
    "print(\"Test MSE scaled:\", mean_squared_error(y_test_scaled, pred_scaled))\n",
    "print(\"Test RMSE scaled:\", np.sqrt(mean_squared_error(y_test_scaled, pred_scaled)))\n",
    "print(\"Test MAE scaled:\", mean_absolute_error(y_test_scaled, pred_scaled))\n",
    "print(\"Test MAPE scaled:\", mean_absolute_percentage_error(y_test_scaled,pred_scaled))\n",
    "print(\"R2 Score scaled:\", r2_score(y_test_scaled, pred_scaled, multioutput='variance_weighted'))\n",
    "print(\"------------------\")\n",
    "#%%\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(y_test, label='Ground truth')\n",
    "plt.plot(pred_unscaled, label='un-scaled Predictions LSTM')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(y_test_scaled, label='Ground truth')\n",
    "plt.plot(pred_scaled, label='scaled Predictions LSTM')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#%%\n",
    "plt.scatter(pred_scaled, y_test)\n",
    "plt.show()\n",
    "\n",
    "# Save the training characteristics\n",
    "#characteristics_file = model_name + \"_characteristics.txt\"\n",
    "#with open(characteristics_file, 'w') as file:\n",
    "#    file.write(\"Loss: {}\\n\".format(history.history['loss'][-1]))\n",
    "    \n",
    "#    file.write(\"Validation Loss: {}\\n\".format(history.history['val_loss'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684c2e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Time per epoch\")\n",
    "print(time_callback.times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ca9e1f",
   "metadata": {},
   "source": [
    "# Training Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570cb964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training characteristics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['accuracy'], label='Training accuracy',linewidth=4)\n",
    "plt.plot(history.history['val_accuracy'], label='Validation accuracy',linewidth=4)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training mse Loss',linewidth=4)\n",
    "plt.plot(history.history['val_loss'], label='Validation mse Loss',linewidth=4)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['MAE'], label='Training MAE',linewidth=4)\n",
    "plt.plot(history.history['val_MAE'], label='Validation MAE',linewidth=4)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['MAPE'], label='Training MAPE',linewidth=4)\n",
    "plt.plot(history.history['val_MAPE'], label='Validation MAPE',linewidth=4)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAPE')\n",
    "#plt.ylim(0, 200)  # Set the y-axis limits\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c0a838",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d237d8f",
   "metadata": {},
   "source": [
    "### Predictions scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a078aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test MSE:\", mean_squared_error(y_test_scaled, pred_scaled,squared=False))\n",
    "print(\"Test RMSE:\", mean_squared_error(y_test_scaled, pred_scaled,squared=True))\n",
    "print(\"Test MAE:\", mean_absolute_error(y_test_scaled, pred_scaled))\n",
    "print(\"Test MAPE:\", mean_absolute_percentage_error(y_test_scaled,pred_scaled))\n",
    "print(\"R2 Score:\", r2_score(y_test_scaled, pred_scaled, multioutput='variance_weighted'))\n",
    "#%%\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title(\"Predictions\")\n",
    "plt.plot(y_test_scaled, label='Ground truth',linewidth=4)\n",
    "plt.plot(pred_scaled, label='scaled Predictions LSTM',linewidth=4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a98577",
   "metadata": {},
   "source": [
    "### Predictions without scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99310936",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test MSE:\", mean_squared_error(y_test, pred_unscaled,squared=False))\n",
    "print(\"Test RMSE:\", mean_squared_error(y_test, pred_unscaled,squared=True))\n",
    "print(\"Test MAE:\", mean_absolute_error(y_test, pred_unscaled))\n",
    "print(\"Test MAPE:\", mean_absolute_percentage_error(y_test,pred_unscaled)*100)\n",
    "print(\"R2 Score:\", r2_score(y_test, pred_unscaled, multioutput='variance_weighted'))\n",
    "#%%\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title(\"Predictions\")\n",
    "plt.plot(y_test, label='Test ',linewidth=4)\n",
    "plt.plot(pred_unscaled, label='Predictions LSTM',linewidth=4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4911401a",
   "metadata": {},
   "source": [
    "# Predictions plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7463704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 7))\n",
    "#plt.plot(np.concatenate((y_train,y_val, y_test)), label='Test ')\n",
    "nanarray = np.full(len(y_train)+len(y_val), np.nan)\n",
    "plt.plot(np.array(np.concatenate((nanarray, np.array(pred_unscaled).flatten()))), label='QLSTM Predictions', linewidth=6, color=\"green\")\n",
    "plt.plot(np.concatenate((y_train,y_val,y_test)), label='Real', linewidth=4, color=\"blue\")\n",
    "plt.ylabel('')\n",
    "plt.xlabel('Hours')\n",
    "plt.vlines(len(np.array(y_train)), ymin = min(y_train), ymax = max(y_train), label = \"val set start\", linestyles = \"dashed\", linewidth=6, color=\"grey\")\n",
    "plt.legend()\n",
    "plt.vlines(len(np.array(np.concatenate((y_train,y_val)))), ymin = min(y_train), ymax = max(y_train), label = \"Test set start\", linestyles = \"dashed\", linewidth=6, color=\"black\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b84c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 7))\n",
    "#plt.plot(np.concatenate((y_train,y_val, y_test)), label='Test ')\n",
    "nanarray = np.full(len(y_val), np.nan)\n",
    "plt.plot(np.array(np.concatenate((nanarray, np.array(pred_unscaled).flatten()))), label='QLSTM Predictions'\n",
    "         , linewidth=6, color=\"green\")\n",
    "plt.plot(np.concatenate((y_val,y_test)), label='Real', linewidth=4, color=\"blue\")\n",
    "plt.ylabel('')\n",
    "plt.xlabel('')\n",
    "plt.vlines(len((y_val)), min(y_train), ymax = max(y_train), label = \"Test set start\", linestyles = \"dashed\", linewidth=5, color=\"black\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d88414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save plots to a file\n",
    "# Create a DataFrame from the arrays\n",
    "import pandas as pd\n",
    "df1 = pd.DataFrame()\n",
    "file_name = model_name + \"_predictions.csv\"\n",
    "arr1=np.concatenate((y_train,y_val,y_test))\n",
    "arr2=np.concatenate((y_train_scaled,y_val_scaled, np.array(pred_scaled).flatten()))\n",
    "arr3=np.concatenate((y_train,y_val, np.array(pred_unscaled).flatten()))\n",
    "#df = pd.DataFrame({'Real': arr1,  'QLSTM': arr2})\n",
    "# Add the array to the specified column\n",
    "df1[\"Q\" + str(n_qubits) + \"B\" + str(blocks) + \"L\" + str(layers)]=np.nan\n",
    "df1[\"Real\"] = arr1\n",
    "df1[\"Pred s\"] = arr2\n",
    "df1[\"Pred\"] = arr3\n",
    "df1[\"----\"]=np.nan\n",
    "df1[\"test MSE s\"]=mean_squared_error(y_test_scaled, pred_scaled, squared=False)\n",
    "df1[\"test RMSE s\"]=mean_squared_error(y_test_scaled, pred_scaled, squared=True)\n",
    "df1[\"test MAE s\"]=mean_absolute_error(y_test_scaled, pred_scaled )\n",
    "df1[\"test MAPE s\"]=mean_squared_error(y_test_scaled, pred_scaled)\n",
    "df1[\"R2 s\"]=r2_score(y_test_scaled, pred_scaled, multioutput='variance_weighted')\n",
    "df1[\"--~--\"]=np.nan\n",
    "df1[\"test MSE\"]=mean_squared_error(y_test, pred_unscaled, squared=False)\n",
    "df1[\"test RMSE\"]=mean_squared_error(y_test, pred_unscaled, squared=True)\n",
    "df1[\"test MAE\"]=mean_absolute_error(y_test, pred_unscaled)\n",
    "df1[\"test MAPE\"]=mean_absolute_percentage_error(y_test, pred_unscaled)\n",
    "df1[\"R2 \"]=r2_score(y_test, pred_unscaled, multioutput='variance_weighted')\n",
    "# Save the DataFrame as a CSV file\n",
    "df1.to_csv(file_name, index=False)\n",
    "print(\"Arrays saved as\", file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39260112",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = model_name + \"_history.csv\"\n",
    "df2 = pd.DataFrame()\n",
    "df2[\"Q\" + str(n_qubits) + \"B\" + str(blocks) + \"L\" + str(layers)]=np.nan\n",
    "df2[\"loss\"]=history.history[\"loss\"]\n",
    "df2[\"val_loss\"]=history.history[\"val_loss\"]\n",
    "df2[\"MAE\"]=history.history[\"MAE\"]\n",
    "df2[\"val_MAE\"]=history.history[\"val_MAE\"]\n",
    "df2[\"MAPE\"]=history.history[\"MAPE\"]\n",
    "df2[\"val_MAPE\"]=history.history[\"val_MAPE\"]\n",
    "df2.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe90622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len y_train + y_val\n",
    "len(np.array(np.concatenate((y_train,y_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36becee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfcdd5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bac73f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
